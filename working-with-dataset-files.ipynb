{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/CC_BY.png\"><br />\n",
    "\n",
    "Created by [Nathan Kelber](http://nkelber.com) and Ted Lawless for [JSTOR Labs](https://labs.jstor.org/) under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/)<br />\n",
    "Modified by [Tyrica Terry Kapral](mailto:tyt3@pitt.edu).<br />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Working with Dataset Files**\n",
    "\n",
    "**Description:** This notebook describes how to:\n",
    "* Read and write files (.txt, .csv, .json)\n",
    "* Use the `tdm_client` to read in metadata\n",
    "* Use the `tdm_client` to read in data\n",
    "\n",
    "This notebook describes how to read and write text, CSV, and JSON files using Python. Additionally, it explains how the `tdm_client` can help users load and analyze their datasets.\n",
    "\n",
    "**Difficulty:** Intermediate\n",
    "\n",
    "**Knowledge Required:** \n",
    "* Python Basics ([Start Python Basics I](./python-basics-1.ipynb))\n",
    "\n",
    "**Knowledge Recommended:** None\n",
    "\n",
    "**Data Format:** Text (.txt), CSV (.csv), JSON (.json), JSON Lines (.jsonl)\n",
    "\n",
    "**Libraries/Packages Used:**\n",
    "* `pandas` to read and write CSV files\n",
    "* `json` to read and write JSON files\n",
    "* `tdm_client` to retrieve and read data\n",
    "* `datetime`\n",
    "* `jsonlines`\n",
    "* `punctuation`, `whitespace` from `string`\n",
    "* `contractions`\n",
    "* `stopwords` from `nltk.corpus`\n",
    "__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Files in Python\n",
    "Working with files is an essential part of Python programming. When we execute code in Python, we manipulate data through the use of variables. When the program is closed, however, any data stored in those variables is erased. To save the information stored in variables, we must learn how to write it to a file.\n",
    "\n",
    "At the same time, we may have notebooks for applying specific analyses, but we need to have a way to bring data into the notebook for analysis. Otherwise, we would have to type all the data into the program ourselves! Both reading-in from files and writing-out data out to files are important skills for data science and the digital humanities.\n",
    "\n",
    "This section describes how to work with three kinds of common data files in Python:\n",
    "* Plain Text Files (.txt)\n",
    "* Comma-Separated Value files (.csv)\n",
    "* Javascript Object Notation files (.json)\n",
    "\n",
    "Each of these filetypes are in wide use in data science, digital humanities, and general programming. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Three Common Data File Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plain Text Files (.txt)\n",
    "A plain text file is one of the simplest kinds of computer files. Plain text files can be opened with a text editor like Notepad (Windows 10) or TextEdit (OS X). The file can contain only basic textual characters such as: letters, numbers, spaces, and line breaks. Plain text files do not contain styling such as: heading sizes, italic, bold, or specialized fonts. (To including styling in a text file, writers may use other file formats such as rich text format (.rtf) or markdown (.md).)\n",
    "\n",
    "Plain text files (.txt) can be easily viewed and modified by humans by changing the text within. This is an important distinction from binary files such as images (.jpg), archives (.gzip), audio (.wav), or video (.mp4). If a binary file is opened with a text editor, the content will be largely unreadable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comma-Separated Value Files (.csv)\n",
    "A comma-separated value file is also a text file that can easily be modifed with a text editor. A CSV file is generally used to store data that fits in a series or table (like a list or spreadsheet). A spreadsheet application (like Microsoft Excel or Google Sheets) will allow you to view and edit a CSV data in the form of a table.\n",
    "\n",
    "Each row of a CSV file represents a single row of a table. The values in a CSV are separated by commas (with no space between), but other delimiters can be chosen such as a tab or pipe (|). A tab-separated value file is called a TSV file (.tsv). Using tabs or pipes may be preferable if the data being stored contains commas (since this could make it confusing whether a comma is part of a single entry or a delimiter between entries).\n",
    "\n",
    "### The text contents of a sample CSV file\n",
    "```\n",
    "Username,Login email,Identifier,First name,Last name\n",
    "booker12,rachel@example.com,9012,Rachel,Booker\n",
    "grey07,,2070,Laura,Grey\n",
    "johnson81,,4081,Craig,Johnson\n",
    "jenkins46,mary@example.com,9346,Mary,Jenkins\n",
    "smith79,jamie@example.com,5079,Jamie,Smith\n",
    "```\n",
    "### The same CSV file represented in Google Sheets:\n",
    "\n",
    "![CSV table view in Google Sheets](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/csv_in_sheets.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JavaScript Object Notation (.json)\n",
    "A Javascript Object Notation file is also a text file that can be modified with a text editor. A JSON file stores data in key/value pairs, very similar to a Python dictionary. One of the key benefits of JSON is its compactness which makes it ideal for exchanging data between web browsers and servers.\n",
    "\n",
    "While smaller JSON files can be opened in a text editor, larger files can be difficult to read. Viewing and editing JSON is easier in specialized editors, available online at sites like: \n",
    "\n",
    "* [JSON Formatter](http://jsonformatter.org)\n",
    "* [JSON Editor Online](https://jsoneditoronline.org/)\n",
    "\n",
    "A JSON file has a nested structure, where smaller concepts are grouped under larger ones. Like extensible markup language (.xml), a JSON file can be checked to determine that it is valid (follows the proper format for JSON) and/or well-formed (follows the proper format defined in a specialized example, called a schema). \n",
    "\n",
    "### The text contents of a sample JSON file\n",
    "\n",
    "```\n",
    "{\n",
    "    \"firstName\": \"Julia\",\n",
    "    \"lastName\": \"Smith\",\n",
    "    \"gender\": \"woman\",\n",
    "    \"age\": 57,\n",
    "    \"address\": {\n",
    "        \"streetAddress\": \"11434\",\n",
    "        \"city\": \"Detroit\",\n",
    "        \"state\": \"Mi\",\n",
    "        \"postalCode\": \"48202\"\n",
    "    },\n",
    "    \"phoneNumbers\": [\n",
    "        { \"type\": \"home\", \"number\": \"7383627627\" }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "### The same JSON file represented in JSON Editor Online\n",
    "![An image of the JSON file showing the structure](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/json_editor.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opening, Reading, and Writing Text Files (.txt)\n",
    "\n",
    "## Open the File\n",
    "\n",
    "Before we can read or write to text file, we must open the file. Normally, when we open a file, a new window appears where we can see the contents. In Python, opening a file simply means create a *file object* that refers to the particular file. When the file has been opened, we can read or write to the file. Finally, we must close the file. Here are the three steps:\n",
    "\n",
    "1. Use the open() function to create a file object\n",
    "2. Use the .read(), .readlines(), or .write() method on the file object\n",
    "3. Use the close() function to close the file object\n",
    "\n",
    "Let's practice on `sample.txt`, a sample text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the text file `sample.txt` creating\n",
    "# a file object called `f`\n",
    "\n",
    "f = open('data/sample.txt', 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created a file object called `f`. The first argument (`'sample.txt'`) is a string containing the file name. You can see the sample.txt in the same directory as this lesson. If your file was called reports.txt, you would replace that argument with `'reports.txt'`. The second argument (`'r'`) determines that we are opening the file in \"read\" mode, where the file can be read but not modified. There are three main modes that can be specified:\n",
    "\n",
    "|Argument|Mode Name|Description|\n",
    "|---|---|---|\n",
    "|'r'|read|Reads file but no writing allowed (protects file from modification)|\n",
    "|'w'|write|Writes to file, overwriting any information in the file (saves over the current version of the file|\n",
    "|'a'|append|Appends (or adds) information to the end of the file (new information is added below old information)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the File\n",
    "\n",
    "### .read() method\n",
    "Now that we have a file object `f` opened in \"read mode,\" let's read the contents with the `.read()` method. We will create a variable called `file_contents` to hold the data that we are reading in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a variable called `file_contents`\n",
    "# that will hold the result of using the\n",
    "# .read() method on our file object\n",
    "file_contents = f.read()\n",
    "print(file_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we are finished with the file object, we must close it using the .close() method on it. It is very important to always close a file, otherwise your program may crash or create memory problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the file by using the .close() method\n",
    "# on the file object\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .readlines() method\n",
    "\n",
    "If a file is very large, we may want to read a single line at a time so as not to fill all of the available computer memory. To read a single line at a time, we can use the `.readlines()` method instead of the `.read()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file sample.txt in read mode\n",
    "# creating the file object `f`\n",
    "f = open('data/sample.txt', 'r')\n",
    "file_contents = f.readlines()\n",
    "print(file_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `.read()` method, we read in the whole text file as a single string. The `.readlines()` gives us a Python list, where each item is a single string representing a single line of the text document. You may also notice that each line ends with `\\n` which represents a line break in a string. If we print the string, the line break is visible in our output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first item in the file_contents list\n",
    "# Note the \\n turns into a line break\n",
    "print(file_contents[0])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to the File\n",
    "To write to a file, we need to open it and create our file object in either write ('w') or append ('a') mode. \n",
    "\n",
    "### Append mode\n",
    "Let's start with append mode which adds new data to the bottom of the file while leaving any previous information intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening a file in append mode\n",
    "# and creating a new file object \n",
    "# called `f`\n",
    "f = open('data/sample.txt', 'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the `.write()` method to append a string to the file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending an eleventh line to the file\n",
    "f.write('\\nThis is the eleventh line')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you read the file back in to see whether the `.write()` was successful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the the file in read mode\n",
    "# create a file object called `sample_file`\n",
    "f = \n",
    "\n",
    "# Use the .read() method on the file object\n",
    "# Store the result in a variable `file_contents`\n",
    "file_contents = \n",
    "\n",
    "# Print the contents\n",
    "print(file_contents)\n",
    "\n",
    "# Close the file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write mode\n",
    "Opening a file in write mode is useful in two scenarios:\n",
    "* Creating a new text file and writing data to it\n",
    "* Overwriting all data in the file with new data\n",
    "\n",
    "Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new file in write mode\n",
    "f = open('data/new_sample.txt', 'w')\n",
    "\n",
    "# Define a string variable to add to the new file\n",
    "string = 'Here is some data\\nWith a second line'\n",
    "\n",
    "# Using write method on the file object\n",
    "contents = f.write(string)\n",
    "\n",
    "# Close file object\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open/Close Files `with open`\n",
    "The `with open` technique is commonly used in Python because it has two significant advantages:\n",
    "* It is more compact \n",
    "* It automatically closes the file afterward\n",
    "\n",
    "The basic form resembles a flow control statement, ending in a colon and then executing an indented block of code. After the block executes, the file is closed automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/sample.txt', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opening, Reading, and Writing CSV Files (.csv)\n",
    "CSV file data can be easily opened, read, and written using the `pandas` library. (For large CSV files (>500 mb), you may wish to use the `csv` library to read in a single row at a time to reduce the memory footprint.) Pandas is flexible for working with tabular data, and the process for importing and exporting to CSV is simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas \n",
    "import pandas as pd\n",
    "\n",
    "# Create our dataframe\n",
    "df = pd.read_csv('data/sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you've made any necessary changes in Pandas, write the dataframe back to the CSV file. (Remember to always back up your data before writing over the file.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write data to new file\n",
    "# Keeping the Header but removing the index\n",
    "df.to_csv('data/new_sample.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opening, Reading, and Writing JSON Files (.json)\n",
    "\n",
    "JSON files use a key/value structure very similar to a Python dictionary. We will start with a Python dictionary called `py_dict` and then write the data to a JSON file using the `json` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining sample data in a Python dictionary\n",
    "py_dict = {\n",
    "    \"firstName\": \"Julia\",\n",
    "    \"lastName\": \"Smith\",\n",
    "    \"gender\": \"woman\",\n",
    "    \"age\": 57,\n",
    "    \"address\": {\n",
    "        \"streetAddress\": \"11434\",\n",
    "        \"city\": \"Detroit\",\n",
    "        \"state\": \"Mi\",\n",
    "        \"postalCode\": \"48202\"\n",
    "    },\n",
    "    \"phoneNumbers\": [\n",
    "        { \"type\": \"home\", \"number\": \"7383627627\" }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To write our dictionary to a JSON file, we will use the `with open` technique we learned that automatically closes file objects. We also need the `json` library to help dump our dictionary data into the file object. The `json.dump` function works a little differently than the write method we saw with text files. \n",
    "\n",
    "We need to specify two arguments: \n",
    "\n",
    "* The data to be dumped\n",
    "* The file object where we are dumping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open/create sample.json in write mode\n",
    "# as the file object `f`. The data in py_dict\n",
    "# is dumped into `f` and then `f` is closed\n",
    "\n",
    "import json\n",
    "with open('data/sample.json', 'w') as f:\n",
    "    json.dump(py_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read data in from a JSON file, we can use the `json.load` function on our file object. Here we load all the content into a variable called `content`. We can then print values based on particular keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/sample.json') as f:\n",
    "    contents = json.load(f)\n",
    "    print('First Name: ' + contents['firstName'])\n",
    "    print('Last Name: '+ contents['lastName'])\n",
    "    print('Age: ' + str(contents['age']))\n",
    "    print('Phone Number: ', contents['phoneNumbers'][0]['number'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opening datasets with `tdm_client`\n",
    "\n",
    "The `tdm_client` helps retrieve a given dataset and/or its associated metadata. The metadata is supplied in a CSV file and the full dataset is supplied in a compressed JSON Lines file (.jsonl.gz). For any analysis focused on metadata pre-processing, we recommend users start with the CSV file since it is both easier and faster to view, parse, and manipulate.\n",
    "\n",
    "\n",
    "## Metadata CSV vs. JSON Lines Data File\n",
    "All of the textual data and metadata is available inside of the JSON Lines files, but we have chosen to offer the metadata CSV for two primary reasons:\n",
    "\n",
    "1. The JSON Lines data is a little more complex to parse since it is nested. It cannot be easily represented in a table form in something like Pandas. It is nice to be able to easily view all the metadata in a Pandas dataframe.\n",
    "2. The JSON Lines data can be very large. Each file contains all of the metadata *plus* unigram counts, bigram counts, trigram counts, and full-text (when available). Manipulating all that data takes significant computer time and costs. Even a modest dataset (~5000 files) can be over 1 GB in size uncompressed.\n",
    "\n",
    "More information is available, including the metadata categories, in the FAQ [\"What is the data file format?\"](https://docs.tdm-pilot.org/what-format-are-jstor-portico-datasets/). \n",
    "\n",
    "## Retrieving data (`tdm_client` methods)\n",
    "\n",
    "By passing the `tdm_client` a dataset ID (here called `dataset_id`), we can automatically download the metadata CSV file or the full JSON Lines dataset created by the [dataset builder](https://tdm-pilot.org/builder/).\n",
    "\n",
    "* Use the `.get_metadata()` method to retrieve the metadata CSV file\n",
    "* Use the `get_dataset()` method to retrieve the full JSON data file\n",
    "\n",
    "|Code|Result|\n",
    "|---|---|\n",
    "|f = tdm_client.get_metadata(dataset_id)|Automatically retrieves a metadata CSV file and creates a file object `f`|\n",
    "|f = tdm_client.get_dataset(dataset_id)| Automatically retrieves a compressed JSON Lines dataset file (jsonl.gz) and creates a file object `f`|\n",
    "\n",
    "The JSON Lines file will be downloaded in a compressed gzip format (jsonl.gz). We can iterate over each document in the corpus by using the `dataset_reader()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the `tdm_client` methods\n",
    "\n",
    "We'll import each of the methods individually so that we don't have to include `tdm_client` each time we call them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules from the `tdm_client`\n",
    "from tdm_client import get_dataset, get_metadata, dataset_reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import your dataset\n",
    "\n",
    "First, we'll use the `tdm_client` library to automatically retrieve the [metadata](https://docs.tdm-pilot.org/key-terms/#metadata) for a [dataset](https://docs.tdm-pilot.org/key-terms/#dataset). We can retrieve [metadata](https://docs.tdm-pilot.org/key-terms/#metadata) in a [CSV file](https://docs.tdm-pilot.org/key-terms/#csv-file) using the `get_metadata` method.\n",
    "\n",
    "Enter a [dataset ID](https://docs.tdm-pilot.org/key-terms/#dataset-ID) in the next code cell. \n",
    "\n",
    "If you don't have a dataset ID, you can:\n",
    "* Use the sample dataset ID already in the code cell\n",
    "* [Create a new dataset](https://tdm-pilot.org/builder)\n",
    "* [Use a dataset ID from other pre-built sample datasets](https://tdm-pilot.org/dataset/dashboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a variable `dataset_id` to hold our dataset ID\n",
    "# The default dataset is \n",
    "dataset_id = \"7e41317e-740f-e86a-4729-20dab492e925\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next pass the `dataset_id` as an argument using the `get_metadata` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in our dataset CSV\n",
    "dataset_metadata = get_metadata(dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to import pandas for our analysis and create a dataframe. We will use the `read_csv()` method to create our dataframe from the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas \n",
    "import pandas as pd\n",
    "\n",
    "# Create our dataframe\n",
    "df = pd.read_csv(dataset_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm the size of our dataset using the `len()` function on our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_document_count = len(df)\n",
    "print('Total original documents:', original_document_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the data in our dataframe `df`. We will set pandas to show all columns using `set_option()` then get a preview using `head()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the pandas option to show all columns\n",
    "pd.set_option(\"max_columns\", None) \n",
    "\n",
    "# Show the first five rows of our dataframe\n",
    "df.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Metadata Type by Column Name\n",
    "\n",
    "Here are descriptions for the metadata types found in each column:\n",
    "\n",
    "|Column Name|Description|\n",
    "|---|---|\n",
    "|id|a unique item ID (In JSTOR, this is a stable URL)|\n",
    "|title|the title for the item|\n",
    "|isPartOf|the larger work that holds this title (for example, a journal title)|\n",
    "|publicationYear|the year of publication|\n",
    "|doi|the digital object identifier for an item|\n",
    "|docType|the type of document (for example, article or book)|\n",
    "|provider|the source or provider of the dataset|\n",
    "|datePublished|the publication date in yyyy-mm-dd format|\n",
    "|issueNumber|the issue number for a journal publication|\n",
    "|volumeNumber|the volume number for a journal publication|\n",
    "|url|a URL for the item and/or the item's metadata|\n",
    "|creator|the author or authors of the item|\n",
    "|publisher|the publisher for the item|\n",
    "|language|the language or languages of the item (eng is the ISO 639 code for English)|\n",
    "|pageStart|the first page number of the print version|\n",
    "|pageEnd|the last page number of the print version|\n",
    "|placeOfPublication|the city of the publisher|\n",
    "|wordCount|the number of words in the item|\n",
    "|pageCount|the number of print pages in the item|\n",
    "|outputFormat|what data is available ([unigrams](https://docs.tdm-pilot.org/key-terms/#unigram), [bigrams](https://docs.tdm-pilot.org/key-terms/#bigram), [trigrams](https://docs.tdm-pilot.org/key-terms/#trigram), and/or full-text)|\n",
    "\n",
    "If there are any columns you would like to drop from your analysis, you can drop them with:\n",
    "\n",
    "`df df.drop(['column_name1', 'column_name2', ...], axis=1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop each of these named columns\n",
    "df = df.drop(['outputFormat', 'pageEnd', 'pageStart', 'datePublished', 'language'], axis=1)\n",
    "\n",
    "# Show the first five rows of our updated dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to know if a particular id is in the dataframe, you can use the `in` operator to return a boolean value (True or False). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if a particular item id is in the `id` column\n",
    "'http://www.jstor.org/stable/2868641' in df.id.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Out Unwanted Texts\n",
    "\n",
    "Now that we have filtered out unwanted metadata columns, we can begin filtering out any texts that may not match our research interests. Let's examine the first and last twenty rows of the dataframe to see if we can identify texts that we would like to remove. We are looking for patterns in the metadata that could help us remove many texts at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the first twenty items in the dataframe\n",
    "# df.head(20) # Change 20 to view a greater or lesser number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the last twenty items in the dataframe\n",
    "# df.tail(20) # Change 20 to view a greater or lesser number of rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove all rows without data for a particular column\n",
    "\n",
    "For example, we may wish to remove any texts that do not have authors. (In the case of journals, this may be helpful for removing paratextual sections such as the table of contents, indices, etc.) The column of interest in this case is `creator`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all texts without an author\n",
    "df = df.dropna(subset=['creator']) #drop each row that has no value under 'creators'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the total original documents followed by the current number\n",
    "print('Total original documents:', original_document_count)\n",
    "print('Total current documents: ', len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove row based on the content of a particular column\n",
    "\n",
    "We can also remove texts that have a particular value in a column. Here are a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all items with a particular title\n",
    "df = df[df.title != 'Review Article'] # Change `Review Article` to your desired title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all items with less than 3000 words\n",
    "df = df[df.wordCount > 3000] # Change `3000` to your desired number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the total original documents followed by the current number\n",
    "print('Total original documents:', original_document_count)\n",
    "print('Total current documents: ', len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a final look at your dataframe to make sure the current texts fit your research goals. In the next step, we will save the IDs of your pre-processed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the first 50 lines of your dataset\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a list of IDs to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the column \"id\" to a CSV file called `pre-processed_###.csv` where ### is the `dataset_id`\n",
    "df[\"id\"].to_csv('data/pre-processed_' + dataset_id + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the \"pre-processed_###.csv\" file (where ### is the `dataset_id`) for future analysis. You can use this file in combination with the dataset ID to automatically filter your texts and reduce the processing time of your analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Visualizing the Pre-Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by publication year and the aggregated number of ids into a bar chart\n",
    "df.groupby(['publicationYear'])['id'].agg('count').plot.bar(title='Documents by year', figsize=(20, 5), fontsize=12); \n",
    "\n",
    "# Read more about Pandas dataframe plotting here: \n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's look at the total page numbers by year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by publication year and aggregated sum of the page counts into a bar chart\n",
    "\n",
    "df.groupby(['publicationYear'])['pageCount'].agg('sum').plot.bar(title='Pages by decade', figsize=(20, 5), fontsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in data from a dataset object\n",
    "\n",
    "The `dataset_reader()` method will read in data from the compressed JSON dataset file object. By keeping the data in a compressed format and reading in a single line at a time, we reduce the processing time and memory use. These can be substantial for large datasets. Even a modest dataset (~5000 files) can be over 1 GB in size uncompressed.\n",
    "\n",
    "The `dataset_reader()` essentially unzips each row of the dataset at a time. Each row constitutes all the metadata and data available for a single document. Here is what that looks like the actual tdm_client:\n",
    "\n",
    "```with gzip.open(file_path, \"rb\") as input_file:\n",
    "        for row in input_file:\n",
    "            yield json.loads(row)\n",
    "```\n",
    "\n",
    "In most cases, users will want to iterate over every document/row in the JSON Lines file. In practice, that looks like:\n",
    "\n",
    "|Code|Result|\n",
    "|---|---|\n",
    "|for document in tdm_client.dataset_reader(f):| Iterates over each document in file object `f`|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in our dataset JSON Lines file\n",
    "f = get_dataset(dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with unigram counts in the dataset file\n",
    "\n",
    "The most significant data for text analysis is usually the \"unigramCount\" section where the frequency of each word is recorded. In this context, the word \"unigram\" describes a single word construction like the word \"chicken.\" \n",
    "\n",
    "\n",
    "In this section we will:\n",
    "- pre-process unigrams\n",
    "- create, explore, and clean a dataframe with unigram counts\n",
    "- export the dataframe to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries for working with dataset files\n",
    "\n",
    "We'll be using several packages for keeping track of execution time, working with JSON Lines files, and text pre-processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import datetime\n",
    "import datetime\n",
    "\n",
    "# Import jsonlines \n",
    "import jsonlines\n",
    "\n",
    "# Import packages for text pre-processing \n",
    "from string import punctuation, whitespace\n",
    "import contractions\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load `preprocess_text` function\n",
    "\n",
    "To simplify the code, we'll go ahead and define the function below to pre-process the unigrams we'll be working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Remove leading/trailing whitespace and punctuation\n",
    "    text = text.strip(whitespace + punctuation)\n",
    "    \n",
    "    # Lowercase all characters\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Expand contractions, split at the new space, and remove auxillary\n",
    "    # You can comment out or delete this step if you want to retain contractions.\n",
    "    text = contractions.fix(text)\n",
    "    text = text.split(\" \", 1)\n",
    "    text = text[0]\n",
    "    \n",
    "    # Return the pre-processed text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a new dataset file with pre-processed unigrams\n",
    "\n",
    "Now we can pre-process the unigrams and create a new dataset file containing and metadata and updated unigram counts. We won't be working with bigrams and trigrams in this workshop, so we will leave those out of the new file, which allows us to work with a smaller file (less time and computational resources required)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer and set counter for progess tracking\n",
    "start_time = datetime.datetime.now()\n",
    "counter = 0\n",
    "\n",
    "# Assign the output file\n",
    "output_file = 'data/%s_%s.jsonl' % (dataset_id, 'unigrams')\n",
    "\n",
    "\n",
    "# Open dataset file for writing\n",
    "with jsonlines.open(output_file, mode='w') as writer:\n",
    "    \n",
    "    # Iterate through documents/lines in JSON Lines file\n",
    "    for document in dataset_reader(f):\n",
    "        new_document = {}\n",
    "        \n",
    "        # Iterate through keys in current document\n",
    "        for key in document.keys():\n",
    "            \n",
    "            # Process unigrams and add/update unigram counts\n",
    "            if key == 'unigramCount':\n",
    "                unigramCount = document[key]\n",
    "                new_unigramCount = {}\n",
    "                \n",
    "                # Iterate through unigram counts\n",
    "                for unigram in unigramCount.keys():\n",
    "                    \n",
    "                    # Pre-process unigram\n",
    "                    new_unigram = preprocess_text(unigram)\n",
    "                    \n",
    "                    # Update existing unigram in dictionary\n",
    "                    if new_unigram in new_unigramCount:\n",
    "                        new_unigramCount[new_unigram] += unigramCount[unigram]\n",
    "                    \n",
    "                    # Add unigram to dictionary\n",
    "                    else:\n",
    "                        new_unigramCount[new_unigram] = unigramCount[unigram]\n",
    "                \n",
    "                # Add unigram count for new document metadata\n",
    "                new_document[key] = new_unigramCount\n",
    "            \n",
    "            # Skip bigramCount, trigramCount, and fulltext keys\n",
    "            elif key in ['bigramCount', 'trigramCount', 'fullText', 'fulltext']:\n",
    "                continue\n",
    "            \n",
    "            # Add all other fields to new document metadata\n",
    "            else:\n",
    "                new_document[key] = document[key]\n",
    "        \n",
    "        # Write document metadata to JSON Lines file\n",
    "        writer.write(new_document)\n",
    "        \n",
    "        # Report count of processed documents\n",
    "        counter += 1\n",
    "        print(\"Documents processed: %s\" % counter, end=\"\\r\")\n",
    "        if counter == 2500:\n",
    "            break\n",
    "\n",
    "# End timer and print execution time\n",
    "end_time = datetime.datetime.now()\n",
    "print(\"Execution Time: \" + str(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a dataframe with unigram counts\n",
    "\n",
    "Next we'll create a dataframe with the ids and unigram counts. This will allow us to more easily explore and visualize the count data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer and set counter for progess tracking\n",
    "start_time = datetime.datetime.now()\n",
    "counter = 0\n",
    "\n",
    "# Create a list to hold each row of document data \n",
    "document_data = []\n",
    "\n",
    "# Open output file for reading in document data, line-by-line\n",
    "with jsonlines.open(output_file) as reader:\n",
    "    for document in reader:\n",
    "        \n",
    "        # Iterate through keys in dictionary containing document data\n",
    "        for key in document.keys():\n",
    "            \n",
    "            # Save id\n",
    "            if key == 'id':\n",
    "                unique_id = document[key]\n",
    "            \n",
    "            # Iterate through unigramCount dictionary and save unigram and count\n",
    "            elif key == 'unigramCount':\n",
    "                unigramCount = document[key]\n",
    "                for unigram in unigramCount:\n",
    "                    count = unigramCount[unigram]\n",
    "                    \n",
    "                    # Add id, unigram, and count to a row in document data list\n",
    "                    row = (unique_id, unigram, count)\n",
    "                    document_data.append(row)\n",
    "    \n",
    "        # Report count of processed documents\n",
    "        counter += 1\n",
    "        print(\"Documents processed: %s\" % counter, end=\"\\r\")\n",
    "\n",
    "# Create document dataframe\n",
    "print(\"\\nCreating dataframe...\")\n",
    "document_df = pd.DataFrame(document_data, columns =['id', 'unigram', 'count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many rows we have and take a look at the first 20 rows of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display length of dataframe\n",
    "print(document_df.size)\n",
    "\n",
    "# Display the first 20 rows of dataframe\n",
    "document_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the dataframe, you may want to sort them so that the most frequent words are at the stop and descend from there. Let's go ahead and do that, then check the first 20 rows again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort dataframe values by count, in descending order\n",
    "# If you want to sort in ascending order, you can change the Boolean value of `ascending` to True\n",
    "document_df_sorted = document_df.sort_values(by=['count'], ascending=False)\n",
    "\n",
    "# Display the first 20 rows of dataframe\n",
    "document_df_sorted.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing empty values and stopwords from unigram counts\n",
    "\n",
    "You'll probably notice that the most common unigrams are empty values and stopwords (commonly used and auxiliary words, such as “the”, “a”, “an”, “in,” “she”). You might want to get rid of those. \n",
    "\n",
    "This step may take a bit longer than the others because it's a bit resource-consuming to check if each stopword exists in the dataframe and then remove all rows containing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer, get length of stopword list, and set counter for progess tracking\n",
    "start_time = datetime.datetime.now()\n",
    "stopwords_count = len(stopwords.words('english'))\n",
    "counter = 0\n",
    "\n",
    "# Remove all rows where unigram is an empty value\n",
    "document_df_sorted.drop(document_df_sorted[ (document_df_sorted['unigram'] == '')].index, inplace=True)\n",
    "\n",
    "# Iterate through stopwords list and remove rows where unigram is a stopword\n",
    "for stopword in stopwords.words('english'):\n",
    "    document_df_sorted.drop(document_df_sorted[ (document_df_sorted['unigram'] == stopword)].index, inplace=True)\n",
    "    \n",
    "    # Increment count and display progress report\n",
    "    counter += 1\n",
    "    print(\"Progress: %6.2f%%\" % ((counter/stopwords_count)*100), end='\\r')\n",
    "\n",
    "# Notify that program has completed successfully\n",
    "print(\"\\nComplete!\")\n",
    "\n",
    "# Stop timer, then calculate and display program execution time\n",
    "end_time = datetime.datetime.now()\n",
    "print(\"Execution Time: \" + str(end_time - start_time))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the first 20 rows of the dataframe one last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_df_sorted.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might also want to remove numbers, certain characters, etc. But we won't worry about that now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving your unigram count data to a CSV file\n",
    "\n",
    "You'll probably want to save your count data to a CSV file to work with later. Let's do that now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write document dataframe to a CSV file\n",
    "print(\"Writing CSV file...\")\n",
    "document_data_csv = open('data/%s_unigrams.csv' % dataset_id, 'w', encoding='utf-8', newline='')\n",
    "document_data_csv.write(document_df.to_csv(index=False))\n",
    "document_data_csv.close()\n",
    "\n",
    "# Notify that program has completed successfully\n",
    "print(\"Complete!\")\n",
    "\n",
    "# Stop timer, then calculate and display program execution time\n",
    "end_time = datetime.datetime.now()\n",
    "print(\"Execution Time: \" + str(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "At this point, you could create a dataframe for unigram counts aross the whole dataset by dropping the ids and aggregating the counts. Then you also use what we learned earlier in this notebook to visualize the counts of the top unigrams in the corpus. If you're interested in them, you could apply what we did with unigrams to the bigrams and/or trigrams. The world is your oyster! \n",
    "\n",
    "If you need any help, feel free to [email me](mailto:tyt3@pitt.edu) or use the following support resources:\n",
    "- [Attend Office Hours for Beta Participants](https://docs.tdm-pilot.org/office-hours-for-beta-participants/)\n",
    "- [Email the Constellate Team](mailto:tdm@ithaka.org)\n",
    "- [Join the Email Group](https://ithaka.groups.io/g/tdm-jstor-portico)\n",
    "- Join the Constellate Slack Channel ([email me](mailto:tyt3@pitt.edu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "312px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
